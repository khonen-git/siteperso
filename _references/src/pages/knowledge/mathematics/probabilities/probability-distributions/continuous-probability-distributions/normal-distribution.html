<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/src/css/main.css">
    <script src="/src/js/scripts.js"></script>
    <script src="/src/js/knowledge/data-analytics/mathematics/probabilities/probability-distributions/continuous-probability-distributions/normal-distribution.js"></script>
    <link rel="stylesheet" href="/src/assets/lib/katex/katex.css">
    <script src="/src/assets/lib/katex/katex.js"></script>
    <script src="/src/assets/lib/katex/contrib/auto-render.js"></script>
    <title>Votre Nom - Data Analyst</title>
</head>
<body>
    <iframe class="header-iframe" src="/src/partials/header.html"></iframe>
    
    <div class="knowledge-container">
        <iframe class="knowledge-iframe" src="/src/partials/knowledge-toctree.html"></iframe>
    
        <main class="knowledge-content">
            <section class="katex normal-distribution__introduction">
                <h1>Loi normale</h1>
                <p>
                    La loi normale, également connue sous le nom de distribution normale ou courbe en cloche, 
                    est l'une des distributions de probabilité les plus importantes en statistiques et en probabilités. 
                    Elle est caractérisée par une courbe symétrique en forme de cloche qui décrit la répartition des 
                    valeurs d'une variable aléatoire continue.
                </p>

            </section>
            
            <section class="katex normal-distribution__definition">
                <h2>Définition</h2>
                <table class="katex normal-distribution__charts" class="probability-distribution__charts">

                </table>
                <table class="katex normal-distribution__features" class="probability-distribution__features">
                    <tr>
                        <th>Notation</th>
                        <td>
                            \( N(\mu, \sigma^2) \)
                        </td>
                    </tr>
                    <tr>
                        <th>Paramètres</th>
                        <td>
                            \( \mu \in \mathbb{R}, \sigma^2 > 0 \)
                        </td>
                    </tr>
                    <tr>
                        <th>Support</th>
                        <td>
                            \( x \in \mathbb{R} \)
                        </td>
                    </tr>
                    <tr>
                        <th>Densité de probabilité</th>
                        <td>
                            \( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \)
                        </td>
                    </tr>
                    <tr>
                        <th>Fonction de répartition</th>
                        <td>
                            \( \Phi\left(\frac{x-\mu}{\sigma}\right) = \frac{1}{2} \left[1 + \text{erf}\left(\frac{x - \mu}{\sigma\sqrt{2}}\right)\right] \)
                        </td>
                    </tr>
                    <tr>
                        <th>Fonction génératrice des moments</th>
                        <td>
                            \( M_X(t) = \exp\left(\mu t + \frac{1}{2}\sigma^2 t^2\right) \)
                        </td>
                    </tr>
                    <tr>
                        <th>Fonction caractéristiques</th>
                        <td>
                            \( \phi_X(t) = \exp\left(i\mu t - \frac{1}{2}\sigma^2 t^2\right) \)
                        </td>
                    </tr>
                    <tr>
                        <th>Quantile</th>
                        <td>
                            \( Q(p) = \mu + \sigma \Phi^{-1}(p) \)
                        </td>
                    </tr>
                    <tr>
                        <th>Moyenne</th>
                        <td>
                            \( \mu \)
                        </td>
                    </tr>
                    <tr>
                        <th>Médiane</th>
                        <td>
                            \( \mu \)
                        </td>
                    </tr>
                    <tr>
                        <th>Mode</th>
                        <td>
                            \( \mu \)
                        </td>
                    </tr>
                    <tr>
                        <th>Variance</th>
                        <td>
                            \( \sigma^2 \)
                        </td>
                    </tr>
                    <tr>
                        <th>Average absolue deviation</th>
                        <td>
                            \( \frac{\sqrt{2}}{\sigma\sqrt{\pi}} \)
                        </td>
                    </tr>
                    <tr>
                        <th>Asymétrie</th>
                        <td>
                            \( 0 \)
                        </td>
                    </tr>
                    <tr>
                        <th>Kurtosis (aplatissement)</th>
                        <td>
                            \( 3 \)
                        </td>
                    </tr>
                    <tr>
                        <th>Kurtosis normalisé</th>
                        <td>
                            \( 0 \)
                        </td>
                    </tr>
                    <tr>
                        <th>Entropie de Shannon</th>
                        <td>
                            \( \frac{1}{2} + \frac{1}{2} \log(2\pi\sigma^2) \)
                        </td>
                    </tr>
                    <tr>
                        <th>Information de Fisher</th>
                        <td>
                            \( \frac{1}{\sigma^2} \)
                        </td>
                    </tr>
                    <tr>
                        <th>Distance de Kullback-Leibler</th>
                        <td>
                            \( D_{KL}(N_0 || N_1) = \log\left(\frac{\sigma_1}{\sigma_0}\right) + \frac{\sigma_0^2 + (\mu_0 - \mu_1)^2}{2\sigma_1^2} - \frac{1}{2} \)
                        </td>
                    </tr>
                </table>                
            </section>
            <section>
                <h2>Propriétés</h2>
                <ul>
                    <li>
                        <strong>Stabilité par additivité :</strong> Si \( X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2) \) et \( X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2) \), alors \( X_1 + X_2 \sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2) \).
                    </li>
                    <li>
                        <strong>Stabilité par linéarité :</strong> Si \( a \geq 0 \) et \(b\) sont deux réels et \(X \sim \mathcal{N}(\mu, \sigma^2)\), alors \( aX + b \sim \mathcal{N}(a\mu + b, a^2\sigma^2) \).
                    </li>
                </ul>
            </section>
            <section>
                <h2>Théorèmes</h2>
                <ul>
                    <li>
                        <strong>Théorème central limite :</strong> Soit \( X_1, X_2, \ldots, X_n \) une suite de variables aléatoires indépendantes et identiquement distribuées avec une moyenne attendue \( \mu \) et une variance \( \sigma^2 > 0 \). Alors, quand \( n \) tend vers l'infini, la distribution de la moyenne normalisée des variables aléatoires \( \bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i \) onverge en distribution vers une loi normale (gaussienne) avec une moyenne \( \mu \) et une variance \( \frac{\sigma^2}{n} \). Plus précisément, la variable aléatoire \( Z_n = \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \) converge en distribution vers une loi normale standard \( N(0,1) \) lorsque \( n \) tend vers l'infini.
                    </li>
                </ul>
            </section>
            <section>
                <h2>Relations avec les autres distributions</h2>
                <div>
                    <h3>Généralisations de la distribution</h3>
                    <ul>
                        <li>
                            Loi stable : Si \(X\) suit une loi stable avec un paramètre de stabilité \(\alpha = 2\) : \( X \sim \mathcal{S}(2, \beta, \gamma, \delta) \), alors \( X \sim \mathcal{N}(\delta, \gamma^2) \). Le paramètre d'asymétrie \(\beta\) n'a aucune influence lorsque \(\alpha = 2\).
                        </li>
                    </ul>
                </div>
                <div>
                    <h3>Distributions Dérivées</h3>
                    <ul>
                        <li>
                            <strong>Loi log-normale :</strong> Si \( X \sim \mathcal{N}(\mu, \sigma^2) \), alors \( Y = e^X \) suit une loi log-normale : \( Y \sim \ln(\mathcal{N}(\mu, \sigma^2)) \).
                        </li>
                        <li>
                            <strong>Loi uniforme (Transformation de Box-Muller) :</strong> Si \( U \) et \( V \) sont deux variables aléatoires indépendantes uniformes sur [0, 1], alors \( X = \sqrt{-2 \ln(U)} \cos(2\pi V) \) et \( Y = \sqrt{-2 \ln(U)} \sin(2\pi V) \) suivent une distribution normale centrée réduite : \( X \sim \mathcal{N}(0, 1) \) et \( Y \sim \mathcal{N}(0, 1) \).
                        </li>
                        <li>
                            <strong>Loi du \( \chi^2 \) :</strong> Si \( X_1, X_2, ..., X_n \) sont \( n \) variables aléatoires indépendantes et normalement distribuées avec une moyenne de 0 et une variance de 1, alors \( Z = \sum_{i=1}^{n} X_i^2 \) suit une loi du \( \chi^2 \) : \( Z \sim \chi^2(n) \).
                        </li>
                        <li>
                            <strong>Loi de Student :</strong> Si \( U \) suit une loi \( \mathcal{N}(0,1) \) et \( V \) suit une loi \( \chi^2(n) \) indépendante de \( U \), alors \( T = \frac{U}{\sqrt{V/n}} \) suit une loi de Student à \( n \) degrés de liberté.
                        </li>
                        <li>
                            <strong>Loi de Fisher :</strong> Si \( U \) suit une loi \( \chi^2(n_1) \) et \( V \) une loi \( \chi^2(n_2) \), alors \( F = \frac{U/n_1}{V/n_2} \) suit une loi de Fisher avec \( (n_1, n_2) \) degrés de liberté.
                        </li>
                        <li>
                            <strong>Loi de Slash :</strong> Si \( X \) est une variable aléatoire normale centrée réduite et \( U \) est une variable uniforme indépendante sur [0,1], alors \( Y = \frac{X}{U} \) suit une loi de Slash.
                        </li>
                        <li>
                            <strong>Loi normale puissance \( p \) :</strong> Pour \( X \) de loi normale centrée réduite et \( p \) un réel positif, \( Y = sign(X) \cdot |X|^p \) suit une distribution de puissance de loi normale.
                        </li>
                        <li>
                            <strong>Loi de Cauchy :</strong> Si \( Z_1 \) et \( Z_2 \) sont des variables normales centrées réduites indépendantes, alors \( X = \frac{Z_1}{Z_2} \) suit une loi de Cauchy.
                        </li>
                        <li>
                            <strong>Loi de Lévy :</strong> Si \( X \sim \mathcal{N}(\mu, \sigma^2) \), alors \( Y = (X - \mu)^{-2} \) suit un loi de Lévy : \( Y \sim Lévy(\mu, \frac{1}{\sigma^2}) \).
                        </li>
                        <li>
                            <strong>Loi normale repliée et loi demi-normale :</strong> Si \( X \sim \mathcal{N}(\mu, \sigma^2) \), alors \( Y = |X| \) suit la loi normale repliée : \( Y \sim |\mathcal{N}(\mu, \sigma^2)| \). De plus, si \(\mu = 0\), alors \( Y = |X| \) suit la loi demi-normale : \( Y \sim |\mathcal{N}(0, \sigma^2)| \).
                        </li>
                        <li>
                            <strong>Loi normale tronquée :</strong> Si \( X \sim \mathcal{N}(\mu, \sigma^2) \) sur un intervalle restreint \([a,b]\) alors \(X\) suit la loi normale tronquée : \( X \sim \mathcal{TN}(\mu, \sigma^2, a, b) \).
                        </li>
                    </ul>
                </div>
                <div>
                    <h3>Cas particuliers</h3>
                    <ul>
                        <li>
                            <strong>Loi normale centrée réduite :</strong> Si \( X \sim \mathcal{N}(0, 1) \), alors \(X\) suit la loi normale centrée réduite.
                        </li>
                    </ul>
                </div>
            </section>
            <section>
                <h2>Inférence statistique</h2>
                <div>
                    <h3>Estimations</h3>
                    <ul>
                        <li>
                            <strong>Estimation de la moyenne : \( \bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i \).</strong>
                        </li>
                        <li>
                            <strong>Estimation de la variance : \( s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2 \).</strong>
                        </li>
                    </ul>
                </div>
                <div>
                    <h3>Intervalles de confiance</h3>
                    <ul>
                        <li>
                            <strong>Intervalle de la moyenne si la variance est connue :</strong> \( \bar{X} \pm z \cdot \frac{\sigma}{\sqrt{n}} \).
                        </li>
                        <li>
                            <strong>Intervalle de la moyenne si la variance n'est pas connue :</strong> \( \bar{X} \pm t \cdot \frac{s}{\sqrt{n}} \).
                        </li>
                        <li>
                            <strong>Intervalle de la variance si la moyenne est connue :</strong> \( \left[ \frac{\sum_{i=1}^{n} (X_i - \mu)^2}{\chi^2_{1-\alpha/2, n}}, \frac{\sum_{i=1}^{n} (X_i - \mu)^2}{\chi^2_{\alpha/2, n}} \right] \).
                        </li>
                        <li>
                            <strong>Intervalle de la variance si la moyenne n'est pas connue :</strong> \( \left[ \frac{(n-1)s^2}{\chi^2_{1-\alpha/2, n-1}}, \frac{(n-1)s^2}{\chi^2_{\alpha/2, n-1}} \right] \).
                        </li>
                    </ul>
                </div>
                <div>
                    <h3>Tests d'hypothèses</h3>
                    <ul>
                        <li>
                            <strong>Test Z pour la moyenne (variance connue) :</strong> \( Z = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}} \)
                        </li>
                        <li>
                            <strong>Test T pour une moyenne (variance inconnue) :</strong> \( T = \frac{\bar{x} - \mu_0}{s / \sqrt{n}} \)
                        </li>
                        <li>
                            <strong>Test du \(\chi^2\) pour la variance :</strong> \( \chi^2 = \frac{\sum_{i=1}^{n} (X_i - \mu)^2}{\sigma_0^2} \) si variance connue sinon \( \chi^2 = \frac{(n-1)s^2}{\sigma_0^2} \)
                        </li>
                        <li>
                            <strong>Test F pour comparer deux variances :</strong> \( F = \frac{s_1^2 / \sigma_1^2}{s_2^2 / \sigma_2^2} \)
                        </li>
                        <li>
                            <strong>Test de Shapiro-Wilk :</strong> Test de normalité adapté pour les petits échantillons.
                        </li>
                        <li>
                            <strong>Test de Kolmogorov-Smirnov :</strong> Test d'adéquation utile dans le cas général.
                        </li>
                        <li>
                            <strong>Test de Anderson-Darling :</strong> Test d'adéquation adapté lorsque les queues de la distribution tendent vers des valeurs extrêmes.
                        </li>
                        <li>
                            <strong>Test T de comparaison de la moyenne :</strong> \( T = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{s^2_p (\frac{1}{n_1} + \frac{1}{n_2})}} \) si les échantions sont indépendants sinon \( T = \frac{\bar{d}}{s_d / \sqrt{n}} \).
                        </li>
                        <li>
                            <strong>Test ANOVA :</strong>
                        </li>
                        <li>
                            <strong>Test de corrélation de Pearson : </strong> \( r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}} \).
                        </li>
                        <li>
                            <strong>Test de régression linéraire :</strong> \( t = \frac{\hat{\beta} - \beta_0}{\text{Erreur standard de } \hat{\beta}} \)
                        </li>
                    </ul>
                </div>
            </section>
            <section>
                <h2>Utilisation</h2>
            </section>
        </main>
    </div>

    <iframe class="footer-iframe" src="/src/partials/footer.html"></iframe>
    
</body>
</html>